{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# This jupyter notebook command inserts matplotlib graphics in \n",
    "# to the workbook\n",
    "%matplotlib inline\n",
    "\n",
    "# Modify these to point to your own files\n",
    "WRF_DIRECTORY = \"../wrf_tutorial_data\"\n",
    "WRF_FILES = [\"wrfout_d01_2019-07-25_04_00_00\",\n",
    "             \"wrfout_d01_2019-07-25_05_00_00\",\n",
    "             \"wrfout_d01_2019-07-25_06_00_00\"]\n",
    "\n",
    "\n",
    "# Do not modify the code below this line\n",
    "#------------------------------------------------------\n",
    "# Turn off annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make sure the environment is good\n",
    "import numpy\n",
    "import cartopy\n",
    "import matplotlib\n",
    "from netCDF4 import Dataset\n",
    "from xarray import DataArray\n",
    "from wrf import (getvar, interplevel, vertcross, \n",
    "                 vinterp, ALL_TIMES)\n",
    "import os\n",
    "\n",
    "_WRF_FILES = [os.path.abspath(\n",
    "    os.path.join(WRF_DIRECTORY, f)) for f in WRF_FILES]\n",
    "\n",
    "# Check that the WRF files exist\n",
    "try:\n",
    "    for f in _WRF_FILES:\n",
    "        if not os.path.exists(f):\n",
    "            raise ValueError(\"{} does not exist. \"\n",
    "                \"Check for typos or incorrect directory.\".format(f))\n",
    "except ValueError as e:\n",
    "     raise e\n",
    "\n",
    "\n",
    "# Create functions so that the WRF files only need\n",
    "# to be specified using the WRF_FILES global above\n",
    "def single_wrf_file():\n",
    "    global _WRF_FILES\n",
    "    return _WRF_FILES[1]\n",
    "\n",
    "def multiple_wrf_files():\n",
    "    global _WRF_FILES\n",
    "    return _WRF_FILES\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the WRF data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from wrf import to_np, getvar, extract_vars, CoordPair, vertcross\n",
    "\n",
    "# get data\n",
    "file_paths = multiple_wrf_files()\n",
    "wrf_files = [Dataset(f) for f in file_paths]\n",
    "\n",
    "# set cache\n",
    "cache = extract_vars(wrf_files, ALL_TIMES, \n",
    "                     (\"P\", \"PSFC\", \"PB\", \"PH\", \"PHB\",\n",
    "                      \"T\", \"QVAPOR\", \"HGT\", \"U\", \"V\",\n",
    "                      \"W\"))\n",
    "\n",
    "# get variables\n",
    "vars = (\"dbz\", \"mdbz\", \"lat\", \"lon\", \"p\", \"pressure\", \"z\")\n",
    "\n",
    "# Explicitly specifying 'cat', but this is the default\n",
    "dbz = getvar(wrf_files, \"dbz\", timeidx=ALL_TIMES, method=\"cat\", cache=cache)\n",
    "z = getvar(wrf_files, 'z', units='km', timeidx=ALL_TIMES, method=\"cat\", cache=cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1. Xarray](https://github.com/pydata/xarray) and [Dask](https://github.com/dask/dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `xarray` is particularly tailored to working with **netCDF** files, which were the source of xarray’s data model, and integrates tightly with dask for **parallel computing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mdbz by ourself\n",
    "mdbz = dbz.max(dim='bottom_top')\n",
    "\n",
    "mdbz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick convective region data\n",
    "convection = mdbz.where(mdbz > 20)\n",
    "convection.sel(Time='2019-07-25 05:00:00').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the difference along time dimension\n",
    "convection.diff(dim='Time').isel(Time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2. ProPlot](https://github.com/lukelbd/proplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the WRF tutorial but using `proplot` instead of `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subplots of cross sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cross section of simulated radar reflectivity:\n",
    "import numpy as np\n",
    "import proplot as plot\n",
    "\n",
    "Z = 10**(dbz/10.)\n",
    "\n",
    "spoint = [118.9, 31.9]\n",
    "epoint = [119.25, 32.3]\n",
    "\n",
    "z_levels = np.linspace(0, 15, 31)\n",
    "z_cross = vertcross(Z, z, wrfin=wrf_files,\n",
    "                      levels = z_levels,\n",
    "                      start_point=CoordPair(lon=spoint[0], lat=spoint[1]),\n",
    "                      end_point=CoordPair(lon=epoint[0], lat=epoint[1]),\n",
    "                      latlon=True, meta=True)\n",
    "\n",
    "dbz_cross = 10.0 * np.log10(z_cross)\n",
    "\n",
    "dbz_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set axis based on the time dimension\n",
    "f, axs = plot.subplots(ncols=dbz_cross.sizes['Time'], share=3)\n",
    "levels = np.linspace(0, 75, 16)\n",
    "\n",
    "# Set the x-ticks to use latitude and longitude labels\n",
    "coord_pairs = to_np(dbz_cross.coords['xy_loc'])\n",
    "x_ticks = np.arange(coord_pairs.shape[0])\n",
    "x_labels = ['({:.2f}, {:.2f})'.format(pair.lon, pair.lat) for pair in to_np(coord_pairs)]\n",
    "axs.set_xticks(x_ticks[::35])\n",
    "axs.set_xticklabels(x_labels[::30], rotation=45)\n",
    "\n",
    "# Set the y-ticks to be height\n",
    "vert_vals = to_np(dbz_cross.coords['vertical'])\n",
    "v_ticks = np.arange(vert_vals.shape[0])\n",
    "axs.set_yticks(v_ticks[::4][:-1])\n",
    "axs.set_yticklabels(vert_vals[::4][:-1].astype(int))\n",
    "\n",
    "# plot\n",
    "for index,ax in enumerate(axs):\n",
    "    dbz_contours = axs[index].contourf(to_np(dbz_cross.isel(Time=index)),\n",
    "                                       cmap='pyart_dbz_r',\n",
    "                                       levels=levels,\n",
    "                                       )\n",
    "    t = dbz_cross.coords['Time']\n",
    "    title = np.datetime_as_string(t[index], unit='s').replace('T', ' ')\n",
    "    ax.format(title=title)\n",
    "\n",
    "# format axis\n",
    "axs.format(ylim=(v_ticks[::4][0],\n",
    "                 v_ticks[::4][-2]),\n",
    "           grid=False,\n",
    "           abc=True,\n",
    "           abcloc='ul',\n",
    "           abcstyle='(a)',\n",
    "           ylabel='Height (km)',\n",
    "           suptitle='WRF-Chem Composite Reflectivity'\n",
    "          )\n",
    "\n",
    "# add colorbar\n",
    "f.colorbar(dbz_contours,\n",
    "           loc='r',\n",
    "           label='(dBZ)',\n",
    "           ticks=levels[::3]\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3. Pandas](https://github.com/pandas-dev/pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Flexible and powerful data analysis / manipulation library for Python,\n",
    "providing **labeled** data structures similar to R data.frame objects, statistical functions, and much more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dbz_cross.to_dataframe()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of dbz along the cross line at specific time and height\n",
    "df_line = df.loc['2019-07-25 05:00:00', 4] # 2 km\n",
    "\n",
    "df_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickview\n",
    "df_line['dbz_cross'].plot(title='dBZ along one line').set(ylabel='dBZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4. xESMF](https://github.com/JiaweiZhuang/xESMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> xESMF is a Python package for regridding.\n",
    "\n",
    "- Powerful: It uses ESMF/ESMPy as backend and can regrid between general curvilinear grids with **all ESMF regridding algorithms**, such as bilinear, conservative and nearest neighbour.\n",
    "- Easy-to-use: It abstracts away ESMF’s complicated infrastructure and provides a simple, high-level API, **compatible with xarray** as well as basic numpy arrays.\n",
    "- Fast: It is **faster** than ESMPy’s original Fortran regridding engine in serial case, and also supports dask for out-of-core, **parallel computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's resample the simulated mdbz (resolution: 600 m) to 4 km which is useful to compare with the satellite data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `xESMF` to any resolution and projection we want.\n",
    "\n",
    "Here's example of regridding simulated NO2 (600 m * 600 m) to one TROPOMI (Sentinel-5p) swath (5.6 km * 3.5 km).\n",
    "\n",
    "**Note: `xESMF` supports conservative regridder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/xesmf_example.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5. Seaborn](https://github.com/mwaskom/seaborn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn is a Python data visualization library based on matplotlib.\n",
    "\n",
    "It provides a **high-level** interface for drawing attractive and informative statistical graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seaborn.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6. Plotly](https://github.com/plotly/plotly.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly's Python graphing library makes **interactive**, publication-quality graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emmmmmmmmmmmmm\n",
    "\n",
    "## .\n",
    "\n",
    "## .\n",
    "\n",
    "## .\n",
    "\n",
    "## .\n",
    "\n",
    "## ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
